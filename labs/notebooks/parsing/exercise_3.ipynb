{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Portuguese Dataset\n",
    "\n",
    "In this exercise you are going to experiment with arc-factored non-projective dependency parsers. The CoNLL-X and CoNLL 2008 shared task datasets (Buchholz and Marsi, 2006; Surdeanu et al., 2008) contain dependency treebanks for 14 languages. In this lab, we are going to experiment with the Portuguese and English datasets. We preprocessed those datasets to exclude all sentences with more than 15 words; this yielded the files:\n",
    "\n",
    "* data/deppars/portuguese train.conll,\n",
    "* data/deppars/portuguese test.conll,\n",
    "* data/deppars/english train.conll,\n",
    "* data/deppars/english test.conll.\n",
    "\n",
    "After importing all the necessary libraries, load the Portuguese dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../../\")\n",
    "import lxmls.parsing.dependency_parser as depp\n",
    "dp = depp.DependencyParser()\n",
    "dp.read_data(\"portuguese\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the statistics which are shown. How many features are there in total?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Features\n",
    "\n",
    "We will now have a close look on the features that can be used in the parser. Examine the file:\n",
    "\n",
    "```lxmls/parsing/dependency features.py.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method takes a sentence and computes a vector of features for each possible arc $\\langle h, m \\rangle$:\n",
    "\n",
    "```python\n",
    "def create_arc_features(self, instance, h, m, add=False):\n",
    "    '''Creates features for arc h-->m.'''\n",
    "```\n",
    "\n",
    "We grouped the features in several subsets, so that we can conduct some ablation experiments:\n",
    "\n",
    "* **Basic** features that look only at the parts-of-speech of the words that can be connected by an arc;\n",
    "* **Lexical** features that also look at these words themselves;\n",
    "* **Distance** features that look at the length and direction of the dependency link (i.e., distance between the two words);\n",
    "* **Contextual** features that look at the context (part-of-speech tags) of the words surrounding h and m.\n",
    "\n",
    "In the default configuration, only the basic features are enabled. The total number of features is the quantity observed in the previous question. With this configuration, train the parser by running 10 epochs of the structured perceptron algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lxmls.parsing.dependency_parser as depp\n",
    "dp = depp.DependencyParser()\n",
    "dp.read_data(\"portuguese\")\n",
    "dp.train_perceptron(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy obtained in the test set? (Note: the shown accuracy is the fraction of words whose parent was correctly predicted.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Features\n",
    "\n",
    "Repeat the previous exercise by subsequently enabling the lexical, distance and contextual features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp.features.use_lexical = True \n",
    "dp.read_data(\"portuguese\") \n",
    "dp.train_perceptron(10) \n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp.features.use_distance = True\n",
    "dp.read_data(\"portuguese\") \n",
    "dp.train_perceptron(10) \n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp.features.use_contextual = True \n",
    "dp.read_data(\"portuguese\") \n",
    "dp.train_perceptron(10)\n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each configuration, write down the number of features and test set accuracies. Observe the improvements obtained when more features were added. Feel free to engineer new features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with MaxEnt\n",
    "\n",
    "Which of the three important inference tasks discussed above (computing the most likely tree, computing the partition function, and computing the marginals) need to be performed in the structured perceptron algorithm? What about a maximum entropy classifier, with stochastic gradient descent? Check your answers by looking at the following two methods in code/dependency parser.py:\n",
    "\n",
    "```python\n",
    "def train_perceptron(self, n_epochs):\n",
    "    ...\n",
    "```\n",
    "\n",
    "```python\n",
    "def train_crf_sgd(self, n_epochs, sigma, eta0 = 0.001):\n",
    "    ...\n",
    "```\n",
    "\n",
    "Repeat the last exercise by training a maximum entropy classifier, with stochastic gradient descent, using $l$ = 0.01 and a initial stepsize of $\\eta_0$ = 0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp.train_crf_sgd(10, 0.01, 0.1)\n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results with those obtained by the perceptron algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Languages\n",
    "\n",
    "Train a parser for English using your favourite learning algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp.read_data(\"english\")\n",
    "dp.train_perceptron(10)\n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted trees are placed in the file data/deppars/english_test.conll.pred. To get a sense of which errors are being made, you can check the sentences that differ from the gold standard (see the data in data/deppars/english_test.conll) and visualize those sentences, e.g., in https://brenocon.com/parseviz/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eisner's Algorithm *(optional)*\n",
    "\n",
    "Implement Eisner’s algorithm for projective dependency parsing. The pseudo-code is shown as Algorithm 13. Implement this algorithm as the function:\n",
    "\n",
    "```python\n",
    "def parse_proj(self, scores):\n",
    "```\n",
    "in file `dependency_decoder.py`. The input is a matrix of arc scores, whose dimension is $(N + 1)$-by-$(N + 1)$, and whose $(h, m)$ entry contains the score $s_\\sigma(h, m)$.\n",
    "\n",
    "In particular, the first row contains the scores for the arcs that depart from the root, and the first column's values, along with the main diagonal, are to be ignored (since no arcs point to the root, and there are no self-pointing arcs). To make your job easier, we provide an implementation of the backtracking part:\n",
    "\n",
    "```python\n",
    "def backtrack_eisner(self, incomplete_backtrack, complete_backtrack, s, t, direction, complete, heads):\n",
    "```\n",
    "\n",
    "so you just need to build complete/incomplete spans and their backtrack pointers and then call.\n",
    "\n",
    "```python\n",
    "    heads = -np.ones(N+1, dtype=int) \n",
    "    self.backtrack_eisner(incomplete_backtrack, complete_backtrack, 0, N, 1, 1,heads)\n",
    "    return heads\n",
    "```\n",
    "to obtain the final parse. To test the algorithm, retrain the parser on the English data (where the trees are actually all projective) by setting the flag dp.projective to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dp = depp.DependencyParser() \n",
    "dp.features.use_lexical = True \n",
    "dp.features.use_distance = True \n",
    "dp.features.use_contextual = True \n",
    "dp.read_data(\"english\") \n",
    "dp.projective = True\n",
    "dp.train_perceptron(10)\n",
    "dp.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the following results:\n",
    "\n",
    "```\n",
    "￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼￼4.2.5\n",
    "Number of sentences: 8044\n",
    "Number of tokens: 80504\n",
    "Number of words: 12202\n",
    "Number of pos: 48\n",
    "Number of features: 338014\n",
    "Epoch 1\n",
    "Training accuracy: 0.835637168541\n",
    "Epoch 2\n",
    "Training accuracy: 0.922426254687\n",
    "Epoch 3\n",
    "Training accuracy: 0.947621628947\n",
    "Epoch 4\n",
    "Training accuracy: 0.960326602521\n",
    "Epoch 5\n",
    "Training accuracy: 0.967689840538\n",
    "Epoch 6\n",
    "Training accuracy: 0.97263631025\n",
    "Epoch 7\n",
    "Training accuracy: 0.97619370285\n",
    "Epoch 8\n",
    "Training accuracy: 0.979209016579\n",
    "Epoch 9\n",
    "Training accuracy: 0.98127569228\n",
    "Epoch 10\n",
    "Training accuracy: 0.981320865519\n",
    "Test accuracy (509 test instances): 0.886732599366\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}